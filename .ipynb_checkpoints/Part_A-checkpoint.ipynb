{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation , Conv2D, Conv3D\n",
    "import os\n",
    "import sys\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd()+\"/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = []\n",
    "sys.argv.append(data_dir+\"cnn_a.py\")\n",
    "sys.argv.append(data_dir+\"train.csv\")\n",
    "sys.argv.append(data_dir+\"test.csv\")\n",
    "sys.argv.append(data_dir+\"output.txt\")\n",
    "train = pd.read_csv(sys.argv[1],header=None)\n",
    "test  = pd.read_csv(sys.argv[2],header=None)\n",
    "train_data = train.iloc[:100,:][0].str.split(expand=True)\n",
    "train_data = train_data.astype(int)\n",
    "\n",
    "# test_data = test[:,].str.split(expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train.shape,y_train.shape)\n",
    "\n",
    "X_train = X_train.reshape(60000,28,28,1)\n",
    "X_test = X_test.reshape(10000,28,28,1)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 32, 32, 1) (100, 10, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_data.iloc[:,:1024].values.reshape(100,32,32,1)\n",
    "Y_train = to_categorical(train_data.iloc[:,-1].values).reshape(100,10,1,1)\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 32, 32, 1) (100, 10, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=3, activation=\"relu\",input_shape=(32,32,1)))\n",
    "model.add(Dense(10,activation=\"softmax\"))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_3 to have shape (30, 30, 10) but got array with shape (10, 1, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-31d1d9570435>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    139\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_3 to have shape (30, 30, 10) but got array with shape (10, 1, 1)"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, validation_data=(X_train, Y_train), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_class = train.iloc[:,-1].values\n",
    "classes = pd.get_dummies(train[1024],prefix=\"class_\")\n",
    "train = pd.concat([train.iloc[:,:-1],classes],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialiseWeights(layer):\n",
    "    weights = []\n",
    "    bias = []\n",
    "    for i in range(1,len(layer)):\n",
    "        weights.append((np.random.rand(layer[i-1]*layer[i]).reshape(layer[i],layer[i-1])-0.5))\n",
    "        bias.append((np.random.rand(layer[i]).reshape(layer[i],1)-0.5))\n",
    "    return (weights,bias)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliseFeatures(X_train):\n",
    "    return (X_train-np.mean(X_train,axis=0))/255;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp((-1)*z))\n",
    "def sigmoid_der(z):\n",
    "    s = sigmoid(z)\n",
    "    return s*(1-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return (2/(1+np.exp((-2)*z)))-1\n",
    "def tanhder(z):\n",
    "    t = tanh(z)\n",
    "    return 1-t*t;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.where(z>0,z,0)\n",
    "def reluder(z):\n",
    "    z[z>0]=1\n",
    "    z[z<0]=0\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z = np.exp(z)\n",
    "    return z/np.sum(z,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(z):\n",
    "    return np.log(1+np.exp(z))\n",
    "def softplusder(z):\n",
    "    return 1/(1+np.exp((-1)*z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(weights,bias,X_train):\n",
    "    a_layer = []\n",
    "    z_layer = []\n",
    "    a_layer.append(X_train.T)\n",
    "    z_layer.append(0)\n",
    "    for i in range(0,len(layer)-2):\n",
    "        z_layer.append(np.matmul(weights[i],a_layer[i])+bias[i])\n",
    "#         a_layer.append(sigmoid(z_layer[i+1]))\n",
    "        a_layer.append(softplus(z_layer[i+1]))\n",
    "#         a_layer.append(relu(z_layer[i+1]))\n",
    "#         a_layer.append(tanh(z_layer[i+1]))\n",
    "    z_layer.append(np.matmul(weights[-1],a_layer[-1])+bias[-1])\n",
    "    a_layer.append(softmax(z_layer[-1]))    \n",
    "    return (a_layer,z_layer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(weights,bias,a_layer,z_layer,X_train,Y_train,iteration,reg):\n",
    "    regul = 1 - (reg*learningRate/Y_train.shape[0])\n",
    "    new_weights = []\n",
    "    new_bias = []\n",
    "    layer_index = len(a_layer)-1\n",
    "    delta = (a_layer[-1]-Y_train.T)/Y_train.shape[0]\n",
    "    new_weights.append(np.subtract(regul*weights[layer_index-1] ,np.multiply((learningRate/iteration),np.matmul(delta,a_layer[layer_index-1].T))))\n",
    "    b = np.sum(delta,axis=1)\n",
    "    new_bias.append(np.subtract(regul*bias[layer_index-1],np.multiply((learningRate/iteration),b.reshape(b.shape[0],1))))\n",
    "    layer_index-=1\n",
    "    while(layer_index>0):\n",
    "        # delta = np.matmul(weights[layer_index].T,delta)*sigmoid_der(z_layer[layer_index])\n",
    "        delta = np.matmul(weights[layer_index].T,delta)*reluder(z_layer[layer_index])\n",
    "        # delta = np.matmul(weights[layer_index].T,delta)*tanhder(z_layer[layer_index])\n",
    "        new_weights.append(np.subtract(regul*weights[layer_index-1] ,np.multiply((learningRate/iteration),np.matmul(delta,a_layer[layer_index-1].T))))\n",
    "        b = np.sum(delta,axis=1)\n",
    "        new_bias.append(np.subtract(regul*bias[layer_index-1],np.multiply((learningRate/iteration),b.reshape(b.shape[0],1))))\n",
    "        layer_index-=1\n",
    "    new_bias.reverse()\n",
    "    new_weights.reverse()\n",
    "    return (new_weights,new_bias) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(Y_pred,Y_actual):\n",
    "    return (-1)*np.sum(Y_actual.T*np.log(Y_pred))/Y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_encode(Y_pred,Y_actual):\n",
    "    Y_pred = Y_pred.T  \n",
    "    Y_actual = Y_actual.T\n",
    "    maxVal = np.max(Y_pred,axis=1).reshape(Y_pred.shape[0],1) \n",
    "    Y_pred = Y_pred-maxVal\n",
    "    Y_pred[Y_pred==0]=1\n",
    "    Y_pred[Y_pred<0]=0\n",
    "    d = np.sum(Y_pred*Y_actual)\n",
    "    return d/Y_actual.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gabor_filter(X_train,kernal):\n",
    "    X_filtered = [ ndi.convolve(X_train[i],kernal) for i in range(X_train.shape[0])]\n",
    "    return np.array(X_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import gabor_kernel\n",
    "from scipy import ndimage as ndi\n",
    "kernels = []\n",
    "for theta in range(4):\n",
    "    theta = theta / 4. * np.pi\n",
    "    for sigma in (1, 3):\n",
    "        for frequency in (0.05, 0.25):\n",
    "            kernel = np.real(gabor_kernel(frequency, theta=theta,sigma_x=sigma, sigma_y=sigma))\n",
    "#             gabor_filter(X_train[:2],k)\n",
    "            kernels.append(kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normaliseFeatures(train.iloc[:-2000,:-10].values)\n",
    "X_train = np.concatenate([X_train,gabor_filter(X_train,kernels[0][1])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft, dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = dct(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normaliseFeatures(train.iloc[:-2000,:-10].values)\n",
    "Y_train = train.iloc[:-2000,-10:].values\n",
    "X_test = normaliseFeatures(train.iloc[-2000:,:-10].values)\n",
    "Y_test = train.iloc[-2000:,-10:].values\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import feature\n",
    "H = feature.hog(X_train[1], orientations=9, pixels_per_cell=(32, 32),cells_per_block=(2, 2), transform_sqrt=True, block_norm=\"L1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([X_train.T,np.array(gabor_filter(X_train,kernels[0][1])).T,np.real(fft(X_train)).T],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normaliseFeatures(train.iloc[:-2000,:-10].values)\n",
    "Y_train = train.iloc[:-2000,-10:].values\n",
    "X_test = normaliseFeatures(train.iloc[-2000:,:-10].values)\n",
    "Y_test = train.iloc[-2000:,-10:].values\n",
    "# X_train = np.concatenate([X_train.T,gabor_filter(X_train,kernels[0][1]).T,np.real(fft(X_train)).T],axis=1)\n",
    "# X_test = np.concatenate([X_test,gabor_filter(X_test,kernels[0][1]),np.real(fft(X_test)).shape],axis=1)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "with open(sys.argv[3]) as f:\n",
    "    param = f.read().split(\"\\n\")\n",
    "    learningType = int(param[0])\n",
    "    learningRate = float(param[1])\n",
    "    maxIteration = int(param[2])\n",
    "    batchSize = int(param[3])\n",
    "    layer = list(map(int,param[4].split(\" \")))\n",
    "    layer.insert(0,X_train.shape[1])\n",
    "    layer.append(Y_train.shape[1])\n",
    "print(learningType,learningRate,maxIteration,batchSize,layer)        \n",
    "k = batchSize\n",
    "l = X_train.shape[0]\n",
    "weights, bias = initialiseWeights(layer)\n",
    "error_value = []\n",
    "o = maxIteration\n",
    "batches = l/k\n",
    "accuracy_test = []\n",
    "accuracy_train = []\n",
    "for i in range(o):\n",
    "    print(i,end=\"\\r\",flush=True)\n",
    "    start_index = int(k*(i%batches))\n",
    "    end_index = int(k*((i%batches)+1))\n",
    "    a_layer , z_layer = feedforward(weights,bias,X_train[start_index:end_index,:])\n",
    "    Y_pred , z = feedforward(weights,bias,X_test)\n",
    "    accuracy_test.append(hot_encode(Y_pred[-1],Y_test.T))\n",
    "#     Y_pred , z = feedforward(weights,bias,X_train)\n",
    "#     accuracy_train.append(hot_encode(Y_pred[-1],Y_train.T))\n",
    "    error_value.append(error(a_layer[-1],Y_train[start_index:end_index,:]))\n",
    "    weights, bias = backpropagate(weights,bias,a_layer,z_layer,X_train[start_index:end_index,:],Y_train[start_index:end_index,:],np.sqrt(1),0)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(error_value)),error_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(accuracy_test)),accuracy_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normaliseFeatures(train.iloc[:-2000,:-10].values)\n",
    "Y_train = train.iloc[:-2000,-10:].values\n",
    "X_test = normaliseFeatures(train.iloc[-2000:,:-10].values)\n",
    "Y_test = train.iloc[-2000:,-10:].values\n",
    "X_train = np.concatenate([X_train,gabor_filter(X_train,kernels[1][1])],axis=1)\n",
    "X_test = np.concatenate([X_test,gabor_filter(X_test,kernels[0][1])],axis=1)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "with open(sys.argv[3]) as f:\n",
    "    param = f.read().split(\"\\n\")\n",
    "    learningType = int(param[0])\n",
    "    learningRate = float(param[1])\n",
    "    maxIteration = int(param[2])\n",
    "    batchSize = int(param[3])\n",
    "    layer = list(map(int,param[4].split(\" \")))\n",
    "    layer.insert(0,X_train.shape[1])\n",
    "    layer.append(Y_train.shape[1])\n",
    "print(learningType,learningRate,maxIteration,batchSize,layer)        \n",
    "k = batchSize\n",
    "l = X_train.shape[0]\n",
    "weights, bias = initialiseWeights(layer)\n",
    "error_value = []\n",
    "o = maxIteration\n",
    "batches = l/k\n",
    "accuracy_test = []\n",
    "accuracy_train = []\n",
    "for i in range(o):\n",
    "    print(i,end=\"\\r\",flush=True)\n",
    "    start_index = int(k*(i%batches))\n",
    "    end_index = int(k*((i%batches)+1))\n",
    "    a_layer , z_layer = feedforward(weights,bias,X_train[start_index:end_index,:])\n",
    "    Y_pred , z = feedforward(weights,bias,X_test)\n",
    "    accuracy_test.append(hot_encode(Y_pred[-1],Y_test.T))\n",
    "#     Y_pred , z = feedforward(weights,bias,X_train)\n",
    "#     accuracy_train.append(hot_encode(Y_pred[-1],Y_train.T))\n",
    "    error_value.append(error(a_layer[-1],Y_train[start_index:end_index,:]))\n",
    "    weights, bias = backpropagate(weights,bias,a_layer,z_layer,X_train[start_index:end_index,:],Y_train[start_index:end_index,:],np.sqrt(1),0)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(error_value)),error_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(accuracy_test)),accuracy_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normaliseFeatures(train.iloc[:-2000,:-10].values)\n",
    "Y_train = train.iloc[:-2000,-10:].values\n",
    "X_test = normaliseFeatures(train.iloc[-2000:,:-10].values)\n",
    "Y_test = train.iloc[-2000:,-10:].values\n",
    "X_train = np.concatenate([X_train,gabor_filter(X_train,kernels[2][1])],axis=1)\n",
    "X_test = np.concatenate([X_test,gabor_filter(X_test,kernels[2][1])],axis=1)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "with open(sys.argv[3]) as f:\n",
    "    param = f.read().split(\"\\n\")\n",
    "    learningType = int(param[0])\n",
    "    learningRate = float(param[1])\n",
    "    maxIteration = int(param[2])\n",
    "    batchSize = int(param[3])\n",
    "    layer = list(map(int,param[4].split(\" \")))\n",
    "    layer.insert(0,X_train.shape[1])\n",
    "    layer.append(Y_train.shape[1])\n",
    "print(learningType,learningRate,maxIteration,batchSize,layer)        \n",
    "k = batchSize\n",
    "l = X_train.shape[0]\n",
    "weights, bias = initialiseWeights(layer)\n",
    "error_value = []\n",
    "o = maxIteration\n",
    "batches = l/k\n",
    "accuracy_test = []\n",
    "accuracy_train = []\n",
    "for i in range(o):\n",
    "    print(i,end=\"\\r\",flush=True)\n",
    "    start_index = int(k*(i%batches))\n",
    "    end_index = int(k*((i%batches)+1))\n",
    "    a_layer , z_layer = feedforward(weights,bias,X_train[start_index:end_index,:])\n",
    "    Y_pred , z = feedforward(weights,bias,X_test)\n",
    "    accuracy_test.append(hot_encode(Y_pred[-1],Y_test.T))\n",
    "#     Y_pred , z = feedforward(weights,bias,X_train)\n",
    "#     accuracy_train.append(hot_encode(Y_pred[-1],Y_train.T))\n",
    "    error_value.append(error(a_layer[-1],Y_train[start_index:end_index,:]))\n",
    "    weights, bias = backpropagate(weights,bias,a_layer,z_layer,X_train[start_index:end_index,:],Y_train[start_index:end_index,:],np.sqrt(1),0)        \n",
    "plt.plot(range(len(error_value)),error_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(accuracy_test)),accuracy_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normaliseFeatures(train.iloc[:-2000,:-10].values)\n",
    "Y_train = train.iloc[:-2000,-10:].values\n",
    "X_test = normaliseFeatures(train.iloc[-2000:,:-10].values)\n",
    "Y_test = train.iloc[-2000:,-10:].values\n",
    "X_train = np.concatenate([X_train,gabor_filter(X_train,kernels[1][1])],axis=1)\n",
    "X_test = np.concatenate([X_test,gabor_filter(X_test,kernels[1][1])],axis=1)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "with open(sys.argv[3]) as f:\n",
    "    param = f.read().split(\"\\n\")\n",
    "    learningType = int(param[0])\n",
    "    learningRate = float(param[1])\n",
    "    maxIteration = int(param[2])\n",
    "    batchSize = int(param[3])\n",
    "    layer = list(map(int,param[4].split(\" \")))\n",
    "    layer.insert(0,X_train.shape[1])\n",
    "    layer.append(Y_train.shape[1])\n",
    "print(learningType,learningRate,maxIteration,batchSize,layer)        \n",
    "k = batchSize\n",
    "l = X_train.shape[0]\n",
    "weights, bias = initialiseWeights(layer)\n",
    "error_value = []\n",
    "o = maxIteration\n",
    "batches = l/k\n",
    "accuracy_test = []\n",
    "accuracy_train = []\n",
    "for i in range(o):\n",
    "    print(i,end=\"\\r\",flush=True)\n",
    "    start_index = int(k*(i%batches))\n",
    "    end_index = int(k*((i%batches)+1))\n",
    "    a_layer , z_layer = feedforward(weights,bias,X_train[start_index:end_index,:])\n",
    "    Y_pred , z = feedforward(weights,bias,X_test)\n",
    "    accuracy_test.append(hot_encode(Y_pred[-1],Y_test.T))\n",
    "#     Y_pred , z = feedforward(weights,bias,X_train)\n",
    "#     accuracy_train.append(hot_encode(Y_pred[-1],Y_train.T))\n",
    "    error_value.append(error(a_layer[-1],Y_train[start_index:end_index,:]))\n",
    "    weights, bias = backpropagate(weights,bias,a_layer,z_layer,X_train[start_index:end_index,:],Y_train[start_index:end_index,:],np.sqrt(1),0)        \n",
    "plt.plot(range(len(error_value)),error_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(accuracy_test)),accuracy_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(X_train[11].reshape(32,32))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import gabor_kernel\n",
    "from scipy import ndimage as ndi\n",
    "kernels = []\n",
    "for theta in range(4):\n",
    "    theta = theta / 4. * np.pi\n",
    "    for sigma in (1, 3):\n",
    "        for frequency in (0.05, 0.25):\n",
    "            kernel = np.real(gabor_kernel(frequency, theta=theta,sigma_x=sigma, sigma_y=sigma))\n",
    "            kernels.append(kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.real(gabor_kernel(2, theta=np.pi/2,sigma_x=1, sigma_y=1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import gabor_kernel\n",
    "from scipy import ndimage as ndi\n",
    "kernels = []\n",
    "for theta in range(4):\n",
    "    theta = theta / 4. * np.pi\n",
    "    for sigma in (1, 3):\n",
    "        for frequency in (0.05, 0.25):\n",
    "            kernel = np.real(gabor_kernel(frequency, theta=theta,sigma_x=sigma, sigma_y=sigma))\n",
    "            gabor_filter(X_train[:2],k)\n",
    "            kernels.append(kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# for i,k in enumerate(kernels):\n",
    "#     print(k.shape)\n",
    "# k = np.real(gabor_kernel(2, theta=np.pi/2,sigma_x=1, sigma_y=1))\n",
    "# print(k.shape)\n",
    "\n",
    "d = ndi.convolve(X_train[11],kernels[1][1])\n",
    "print(d)\n",
    "plt.imshow(d.reshape(32,32),cmap=\"gray\")\n",
    "plt.show()\n",
    "plt.imshow(X_train[11].reshape(32,32),cmap=\"gray\")\n",
    "plt.show()\n",
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.real(gabor_kernel(4, theta=np.pi/4,sigma_x=2, sigma_y=2)).shape\n",
    "gabor_filter(X_train[:2],k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = ndi.convolve(X_train[1],kernel[6])\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(d.reshape(32,32))\n",
    "plt.show()\n",
    "plt.imshow(X_train[1].reshape(32,32),cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.iloc[1,:-10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = normaliseFeatures(train.iloc[:-2000,:-10].values)\n",
    "# Y_train = train.iloc[:-2000,-10:].values\n",
    "# X_test = normaliseFeatures(train.iloc[-2000:,:-10].values)\n",
    "# Y_test = train.iloc[-2000:,-10:].values\n",
    "\n",
    "# learningType = 2\n",
    "# learningRate = float(0.5)\n",
    "# maxIteration = int(1000)\n",
    "# batchSize = int(100)\n",
    "# layer = [300 , 150, 90]\n",
    "# layer.insert(0,X_train.shape[1])\n",
    "# layer.append(Y_train.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(accuracy_test)),accuracy_test)\n",
    "plt.show\n",
    "plt.plot(range(len(accuracy_train)),accuracy_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer , z_layer = feedforward(weights,bias,X_test)\n",
    "hot_encode(a_layer[-1],Y_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer , z_layer = feedforward(weights,bias,X_test)\n",
    "\n",
    "# a = np.array([ np.where(out==np.amax(out))[0][0] for out in a_layer[-1].T])\n",
    "# j=0\n",
    "# for i in range(a.shape[0]):\n",
    "#     if(a[i]==train_class[i+18000]):\n",
    "#         j+=1\n",
    "# print(j/a.shape[0])\n",
    "# plt.hist(a)\n",
    "# plt.show()\n",
    "# plt.plot(range(len(error_value)),error_value)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer , z_layer = feedforward(weights,bias,X_test)\n",
    "a = np.array([ np.where(out==np.amax(out))[0][0] for out in a_layer[-1].T])\n",
    "j=0\n",
    "for i in range(a.shape[0]):\n",
    "    if(a[i]==train_class[i]):\n",
    "        j+=1\n",
    "print(j/a.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer , z_layer = feedforward(weights,bias,X_train)\n",
    "a = np.array([ np.where(out==np.amax(out))[0][0]+1 for out in a_layer[-1].T])\n",
    "j=0\n",
    "for i in range(a.shape[0]):\n",
    "    if(a[i]==train_class[i]):\n",
    "        j+=1\n",
    "j/a.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(error_value)),error_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([])\n",
    "for m in range(len(bias)):\n",
    "    a = np.concatenate((a,bias[m].flatten(),weights[m].flatten('F')))\n",
    "np.savetxt(\"weigth.txt\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_der = -Y_train.T/(a_layer[-1]*Y_train.shape[0])\n",
    "error_der.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = softmax(z_layer[-1])\n",
    "(s*(1-s)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_laye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer[2][a_layer[2]>0.5]=1\n",
    "a_layer[2][a_layer[2]<0.5]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer[2].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'pred':a_layer[2].tolist(),'actual':Y_train.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(a_layer[2],Y_train,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(weights,bias,a_layer,z_layer,X_train,Y_train):\n",
    "    new_weights = []\n",
    "    new_bias = []\n",
    "    error_der = (a_layer[2]-Y_train)/((1-a_layer[2])*a_layer[2]*X_train.shape[0])\n",
    "    s = sigmoid(z_layer[2])\n",
    "    sigmaDer = (s*(1-s))\n",
    "    deltaL = error_der*sigmaDer\n",
    "    new_weights.append(np.subtract(weights[1] ,np.multiply(learningRate,np.matmul(deltaL,a_layer[1].T))))\n",
    "    b = np.sum(deltaL,axis=1)\n",
    "    new_bias.append(np.subtract(bias[1],np.multiply(learningRate,b.reshape(b.shape[0],1))))\n",
    "    s = sigmoid(z_layer[1])\n",
    "    sigmaDer = (s*(1-s))\n",
    "    deltal3 = np.matmul(weights[1].T,deltaL)*sigmaDer\n",
    "    new_weights.append(np.subtract(weights[0] ,np.multiply(learningRate,np.matmul(deltal3,a_layer[0].T))))\n",
    "    b = np.sum(deltal3,axis=1)\n",
    "    new_bias.append(np.subtract(bias[0],np.multiply(learningRate,b.reshape(b.shape[0],1))))\n",
    "    new_bias.reverse()\n",
    "    new_weights.reverse()\n",
    "    return (new_weights,new_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k = batchSize\n",
    "l = X_train.shape[0]\n",
    "maxIteration =30\n",
    "weights, bias = initialiseWeights(layer)\n",
    "error = []\n",
    "for i in range(maxIteration):\n",
    "    start_index = int((k)*(i%6))\n",
    "    end_index = int((k)*((i%6)+1))\n",
    "    a_layer , z_layer = feedforward(weights,bias,X_train[start_index:end_index,:])\n",
    "    weights, bias = backpropagate(weights,bias,a_layer,z_layer,X_train[start_index:end_index,:],Y_train[start_index:end_index])\n",
    "# a_layer , z_layer = feedforward(weights,bias,X_train)\n",
    "# error.append(np.linalg.norm(np.subtract(a_layer[2],Y_train))/300)   \n",
    "for i in range(len(bias)):\n",
    "    print(bias[i].flatten())\n",
    "print(\"errr\")\n",
    "for i in range(len(weights)):\n",
    "    print(weights[i].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer , z_layer = feedforward(weights,bias,X_train[0:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = []\n",
    "new_bias = []\n",
    "# error_der = (a_layer[2]-Y_train)/((1-a_layer[2])*a_layer[2]*X_train.shape[0])\n",
    "# s = sigmoid(z_layer[2])\n",
    "# sigmaDer = (s*(1-s))\n",
    "# deltaL = error_der*sigmaDer\n",
    "error_der = a_layer\n",
    "# print(deltaL.shape)\n",
    "# new_weights.append(np.subtract(weights[1] ,np.multiply(learningRate,np.matmul(deltaL,a_layer[1].T))))\n",
    "# b = np.sum(deltaL,axis=1)\n",
    "# new_bias.append(b.reshape(b.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias = initialiseWeights(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "k = batchSize\n",
    "l = X_train.shape[0]\n",
    "maxIteration =1\n",
    "weights, bias = initialiseWeights(layer)\n",
    "error = []\n",
    "for j in range(maxIteration):\n",
    "#     print(j,end=\"\\r\",flush=True)\n",
    "    for i in range(int(l/k)):\n",
    "        start_index = int((k)*i)\n",
    "        end_index = int((k)*(i+1))\n",
    "        a_layer , z_layer = feedforward(weights,bias,X_train[start_index:end_index,:])\n",
    "        weights, bias = backpropagate(weights,bias,a_layer,z_layer,X_train[start_index:end_index,:],Y_train[start_index:end_index])\n",
    "#         for k in range(len(bias)):\n",
    "#             print(bias[k].flatten(),weights[k].flatten())\n",
    "#         print(\"hi this is akshay\\n\\n\\n\")\n",
    "    a_layer , z_layer = feedforward(weights,bias,X_train)\n",
    "    error.append(np.linalg.norm(np.subtract(a_layer[2],Y_train))/300)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(error)),error)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer , z_layer = feedforward(weights,bias,X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_layer[2]+Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias = initialiseWeights(layer)\n",
    "a_layer , z_layer = feedforward(weights,bias,X_train[0:50])\n",
    "weights, bias = backpropagate(weights,bias,a_layer,z_layer,X_train[0:50],Y_train[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bias)):\n",
    "    print(bias[i].flatten())\n",
    "print(\"errr\")\n",
    "for i in range(len(weights)):\n",
    "    print(weights[i].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am = [10 , 20 ,30]\n",
    "40 in am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = []\n",
    "new_bias = []\n",
    "error_der = (a_layer[2]-Y_train)/((1-a_layer[2])*a_layer[2]*X_train.shape[0])\n",
    "s = sigmoid(z_layer[2])\n",
    "sigmaDer = (s*(1-s))\n",
    "deltaL = error_der*sigmaDer\n",
    "print(deltaL.shape)\n",
    "new_weights.append(np.subtract(weights[1] ,np.multiply(learningRate,np.matmul(deltaL,a_layer[1].T))))\n",
    "b = np.sum(deltaL,axis=1)\n",
    "new_bias.append(b.reshape(b.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(new_weights[-1])\n",
    "s = sigmoid(z_layer[1])\n",
    "sigmaDer = (s*(1-s))\n",
    "deltal3 = np.matmul(weights[1].T,deltaL)*sigmaDer\n",
    "new_weights.append(np.subtract(weights[0] ,np.multiply(learningRate,np.matmul(deltal3,a_layer[0].T))))\n",
    "b = np.sum(deltal3,axis=1)\n",
    "new_bias.append(b.reshape(b.shape[0],1))\n",
    "# print(new_weights[0].shape,new_weights[1].shape)\n",
    "# print(np.sum(deltaL),deltal3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bias.reverse()\n",
    "new_weights.reverse()\n",
    "print(new_bias,\"\\n\",new_weights)\n",
    "weights = new_weights\n",
    "bias = new_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sigmoid(z_layer[1])\n",
    "sigmaDer = (s*(1-s))\n",
    "deltal3 = np.matmul(weights[1].T,deltaL)*sigmaDer\n",
    "print(weights[1].shape,deltaL.shape,sigmaDer.shape)\n",
    "new_bias.append(np.sum(deltal3,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(weights[1].T,deltaL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = []\n",
    "new_bias = []\n",
    "error_der = (a_layer[4]-Y_train)/((1-a_layer[4])*a_layer[4]*X_train.shape[0])\n",
    "s = sigmoid(z_layer[4])\n",
    "sigmaDer = (s*(1-s))\n",
    "deltaL = error_der*sigmaDer\n",
    "print(deltaL.shape)\n",
    "new_weights.append(np.subtract(weights[3] ,np.multiply(learningRate,np.matmul(deltaL,a_layer[3].T))))\n",
    "print(new_weights[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sigmoid(z_layer[3])\n",
    "sigmaDer = (s*(1-s))\n",
    "deltal3 = np.matmul(weights[3].T,deltaL)*sigmaDer\n",
    "new_weights.append(np.subtract(weights[2] ,np.multiply(learningRate,np.matmul(deltal3,a_layer[2].T))))\n",
    "print(new_weights[-1])\n",
    "# print(weights[2].shape,deltal3.shape,a_layer[2].T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sigmoid(z_layer[2])\n",
    "sigmaDer = (s*(1-s))\n",
    "deltal2 = np.matmul(weights[2].T,deltal3)*sigmaDer\n",
    "new_weights.append(np.subtract(weights[1] ,np.multiply(learningRate,np.matmul(deltal2,a_layer[1].T))))\n",
    "print(new_weights[-1])\n",
    "# print(weights[2].shape,deltal3.shape,a_layer[2].T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sigmoid(z_layer[1])\n",
    "sigmaDer = (s*(1-s))\n",
    "deltal1 = np.matmul(weights[1].T,deltal2)*sigmaDer\n",
    "new_weights.append(np.subtract(weights[0] ,np.multiply(learningRate,np.matmul(deltal1,a_layer[0].T))))\n",
    "print(new_weights[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltal3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.negative(deltaL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = X_train.T\n",
    "z1 = np.matmul(weights[0],y0)+bias[0]\n",
    "y1 = sigmoid(z1)\n",
    "z2 = np.matmul(weights[1],y1)+bias[1]\n",
    "y2 = sigmoid(z2)\n",
    "z3 = np.matmul(weights[2],y2)+bias[2]\n",
    "y3 = sigmoid(z3)\n",
    "z4 = np.matmul(weights[3],y3)+bias[3]\n",
    "y4 = sigmoid(z4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y0.shape,y1.shape,y2.shape,y3.shape,y4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "train = pd.read_csv(sys.argv[1],header=None)\n",
    "test  = pd.read_csv(sys.argv[2],header=None)\n",
    "classes = pd.get_dummies(train[1024],prefix=\"class_\")\n",
    "train = pd.concat([train.iloc[:,:-1],classes],axis=1)\n",
    "def initialiseWeights(layer):\n",
    "    weights = []\n",
    "    bias = []\n",
    "    for i in range(1,len(layer)):\n",
    "        weights.append((np.random.rand(layer[i-1]*layer[i]).reshape(layer[i],layer[i-1])-0.5))\n",
    "        bias.append((np.random.rand(layer[i]).reshape(layer[i],1)-0.5))\n",
    "    return (weights,bias)    \n",
    "def normaliseFeatures(X_train):\n",
    "    return (X_train-np.mean(X_train,axis=0))/255;\n",
    "X_train = normaliseFeatures(train.iloc[:,:-10].values)\n",
    "Y_train = train.iloc[:,-10:].values\n",
    "X_test  = test.iloc[:,:-1].values\n",
    "print(X_train.shape,Y_train.shape,X_test.shape)\n",
    "learningType = 2\n",
    "learningRate = float(0.01)\n",
    "maxIteration = int(1000)\n",
    "batchSize = int(100)\n",
    "layer = [10]\n",
    "layer.insert(0,X_train.shape[1])\n",
    "layer.append(Y_train.shape[1])\n",
    "print(learningType,learningRate,maxIteration,batchSize,layer)\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp((-1)*z))\n",
    "def sigmoid_der(z):\n",
    "    s = sigmoid(z)\n",
    "    return s*(1-s)\n",
    "def tanh(z):\n",
    "    return (2/(1+np.exp((-2)*z)))-1\n",
    "def tanhder(z):\n",
    "    t = tanh(z)\n",
    "    return 1-t*t;\n",
    "def relu(z):\n",
    "    return np.where(z>0,z,0)\n",
    "def reluder(z):\n",
    "    z[z>0]=1\n",
    "    z[z<0]=0\n",
    "    return z\n",
    "def softmax(z):\n",
    "    z = np.exp(z)\n",
    "    return z/np.sum(z,axis=0)    \n",
    "def feedforward(weights,bias,X_train):\n",
    "    a_layer = []\n",
    "    z_layer = []\n",
    "    a_layer.append(X_train.T)\n",
    "    z_layer.append(0)\n",
    "    for i in range(0,len(layer)-2):\n",
    "        z_layer.append(np.matmul(weights[i],a_layer[i])+bias[i])\n",
    "#         a_layer.append(sigmoid(z_layer[i+1]))\n",
    "        a_layer.append(relu(z_layer[i+1]))\n",
    "#         a_layer.append(tanh(z_layer[i+1]))\n",
    "    z_layer.append(np.matmul(weights[-1],a_layer[-1])+bias[-1])\n",
    "    a_layer.append(softmax(z_layer[-1]))    \n",
    "    return (a_layer,z_layer)    \n",
    "def backpropagate(weights,bias,a_layer,z_layer,X_train,Y_train,iteration):\n",
    "    new_weights = []\n",
    "    new_bias = []\n",
    "    layer_index = len(a_layer)-1\n",
    "    delta = (a_layer[-1]-Y_train.T)/Y_train.shape[0]\n",
    "    new_weights.append(np.subtract(weights[layer_index-1] ,np.multiply((learningRate/iteration),np.matmul(delta,a_layer[layer_index-1].T))))\n",
    "    b = np.sum(delta,axis=1)\n",
    "    new_bias.append(np.subtract(bias[layer_index-1],np.multiply((learningRate/iteration),b.reshape(b.shape[0],1))))\n",
    "    layer_index-=1\n",
    "    while(layer_index>0):\n",
    "#         delta = np.matmul(weights[layer_index].T,delta)*sigmoid_der(z_layer[layer_index])\n",
    "        delta = np.matmul(weights[layer_index].T,delta)*reluder(z_layer[layer_index])\n",
    "#         delta = np.matmul(weights[layer_index].T,delta)*tanhder(z_layer[layer_index])\n",
    "        new_weights.append(np.subtract(weights[layer_index-1] ,np.multiply((learningRate/iteration),np.matmul(delta,a_layer[layer_index-1].T))))\n",
    "        b = np.sum(delta,axis=1)\n",
    "        new_bias.append(np.subtract(bias[layer_index-1],np.multiply((learningRate/iteration),b.reshape(b.shape[0],1))))\n",
    "        layer_index-=1\n",
    "    new_bias.reverse()\n",
    "    new_weights.reverse()\n",
    "    return (new_weights,new_bias)    \n",
    "k = batchSize\n",
    "l = X_train.shape[0]\n",
    "weights, bias = initialiseWeights(layer)\n",
    "error_value = []\n",
    "o = maxIteration\n",
    "batches = l/k\n",
    "for i in range(o):\n",
    "    start_index = int(k*(i%batches))\n",
    "    end_index = int(k*((i%batches)+1))\n",
    "    a_layer , z_layer = feedforward(weights,bias,X_train[start_index:end_index,:])\n",
    "    # error_value.append(error(a_layer[-1],Y_train[start_index:end_index,:]))\n",
    "    weights, bias = backpropagate(weights,bias,a_layer,z_layer,X_train[start_index:end_index,:],Y_train[start_index:end_index,:],np.sqrt(1))        \n",
    "    \n",
    "a_layer , z_layer = feedforward(weights,bias,X_test)\n",
    "a = np.array([ np.where(out==np.amax(out))[0][0] for out in a_layer[-1].T])\n",
    "np.savetxt(sys.argv[3],a)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
