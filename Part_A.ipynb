{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation , Conv2D, Conv3D,Flatten , MaxPool2D\n",
    "import os\n",
    "import sys\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd()+\"/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = []\n",
    "sys.argv.append(data_dir+\"cnn_a.py\")\n",
    "sys.argv.append(data_dir+\"train.csv\")\n",
    "sys.argv.append(data_dir+\"test.csv\")\n",
    "sys.argv.append(data_dir+\"output.txt\")\n",
    "train = pd.read_csv(sys.argv[1],header=None)\n",
    "test  = pd.read_csv(sys.argv[2],header=None)\n",
    "\n",
    "# test_data = test[:,].str.split(expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train.iloc[:10000,:][0].str.split(expand=True)\n",
    "train_data = train_data.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(60000, 28, 28, 1) (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train.shape,y_train.shape)\n",
    "\n",
    "X_train = X_train.reshape(60000,28,28,1)\n",
    "X_test = X_test.reshape(10000,28,28,1)\n",
    "Y_train = to_categorical(y_train)\n",
    "Y_test = to_categorical(y_test)\n",
    "print(X_train.shape,Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 32, 32, 3) (9000, 10) (1000, 32, 32, 3) (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "k = 9000\n",
    "X_train = train_data.iloc[:k,:-1].values.reshape(k,32,32,3)\n",
    "Y_train = to_categorical(train_data.iloc[:k,-1].values).reshape(k,10)\n",
    "X_test = train_data.iloc[k:,:-1].values.reshape(total-k,32,32,3)\n",
    "Y_test = to_categorical(train_data.iloc[k:,-1].values).reshape(total-k,10)\n",
    "\n",
    "print(X_train.shape, Y_train.shape, X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, Y_train.shape, X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(32,32,3)))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation=\"relu\"))\n",
    "model.add(Dense(256,activation=\"relu\"))\n",
    "model.add(Dense(10,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 32, 32, 3) (9000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/9\n",
      "9000/9000 [==============================] - 21s 2ms/step - loss: 14.4669 - acc: 0.1024 - val_loss: 14.3773 - val_acc: 0.1080\n",
      "Epoch 2/9\n",
      "9000/9000 [==============================] - 20s 2ms/step - loss: 14.4669 - acc: 0.1024 - val_loss: 14.3773 - val_acc: 0.1080\n",
      "Epoch 3/9\n",
      "9000/9000 [==============================] - 22s 2ms/step - loss: 14.4669 - acc: 0.1024 - val_loss: 14.3773 - val_acc: 0.1080\n",
      "Epoch 4/9\n",
      "6600/9000 [=====================>........] - ETA: 6s - loss: 14.4599 - acc: 0.1029"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=9,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, validation_data=(X_train, Y_train), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_class = train.iloc[:,-1].values\n",
    "classes = pd.get_dummies(train[1024],prefix=\"class_\")\n",
    "train = pd.concat([train.iloc[:,:-1],classes],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialiseWeights(layer):\n",
    "    weights = []\n",
    "    bias = []\n",
    "    for i in range(1,len(layer)):\n",
    "        weights.append((np.random.rand(layer[i-1]*layer[i]).reshape(layer[i],layer[i-1])-0.5))\n",
    "        bias.append((np.random.rand(layer[i]).reshape(layer[i],1)-0.5))\n",
    "    return (weights,bias)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliseFeatures(X_train):\n",
    "    return (X_train-np.mean(X_train,axis=0))/255;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp((-1)*z))\n",
    "def sigmoid_der(z):\n",
    "    s = sigmoid(z)\n",
    "    return s*(1-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return (2/(1+np.exp((-2)*z)))-1\n",
    "def tanhder(z):\n",
    "    t = tanh(z)\n",
    "    return 1-t*t;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.where(z>0,z,0)\n",
    "def reluder(z):\n",
    "    z[z>0]=1\n",
    "    z[z<0]=0\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z = np.exp(z)\n",
    "    return z/np.sum(z,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(z):\n",
    "    return np.log(1+np.exp(z))\n",
    "def softplusder(z):\n",
    "    return 1/(1+np.exp((-1)*z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(weights,bias,X_train):\n",
    "    a_layer = []\n",
    "    z_layer = []\n",
    "    a_layer.append(X_train.T)\n",
    "    z_layer.append(0)\n",
    "    for i in range(0,len(layer)-2):\n",
    "        z_layer.append(np.matmul(weights[i],a_layer[i])+bias[i])\n",
    "#         a_layer.append(sigmoid(z_layer[i+1]))\n",
    "        a_layer.append(softplus(z_layer[i+1]))\n",
    "#         a_layer.append(relu(z_layer[i+1]))\n",
    "#         a_layer.append(tanh(z_layer[i+1]))\n",
    "    z_layer.append(np.matmul(weights[-1],a_layer[-1])+bias[-1])\n",
    "    a_layer.append(softmax(z_layer[-1]))    \n",
    "    return (a_layer,z_layer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(weights,bias,a_layer,z_layer,X_train,Y_train,iteration,reg):\n",
    "    regul = 1 - (reg*learningRate/Y_train.shape[0])\n",
    "    new_weights = []\n",
    "    new_bias = []\n",
    "    layer_index = len(a_layer)-1\n",
    "    delta = (a_layer[-1]-Y_train.T)/Y_train.shape[0]\n",
    "    new_weights.append(np.subtract(regul*weights[layer_index-1] ,np.multiply((learningRate/iteration),np.matmul(delta,a_layer[layer_index-1].T))))\n",
    "    b = np.sum(delta,axis=1)\n",
    "    new_bias.append(np.subtract(regul*bias[layer_index-1],np.multiply((learningRate/iteration),b.reshape(b.shape[0],1))))\n",
    "    layer_index-=1\n",
    "    while(layer_index>0):\n",
    "        # delta = np.matmul(weights[layer_index].T,delta)*sigmoid_der(z_layer[layer_index])\n",
    "        delta = np.matmul(weights[layer_index].T,delta)*reluder(z_layer[layer_index])\n",
    "        # delta = np.matmul(weights[layer_index].T,delta)*tanhder(z_layer[layer_index])\n",
    "        new_weights.append(np.subtract(regul*weights[layer_index-1] ,np.multiply((learningRate/iteration),np.matmul(delta,a_layer[layer_index-1].T))))\n",
    "        b = np.sum(delta,axis=1)\n",
    "        new_bias.append(np.subtract(regul*bias[layer_index-1],np.multiply((learningRate/iteration),b.reshape(b.shape[0],1))))\n",
    "        layer_index-=1\n",
    "    new_bias.reverse()\n",
    "    new_weights.reverse()\n",
    "    return (new_weights,new_bias) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(Y_pred,Y_actual):\n",
    "    return (-1)*np.sum(Y_actual.T*np.log(Y_pred))/Y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_encode(Y_pred,Y_actual):\n",
    "    Y_pred = Y_pred.T  \n",
    "    Y_actual = Y_actual.T\n",
    "    maxVal = np.max(Y_pred,axis=1).reshape(Y_pred.shape[0],1) \n",
    "    Y_pred = Y_pred-maxVal\n",
    "    Y_pred[Y_pred==0]=1\n",
    "    Y_pred[Y_pred<0]=0\n",
    "    d = np.sum(Y_pred*Y_actual)\n",
    "    return d/Y_actual.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gabor_filter(X_train,kernal):\n",
    "    X_filtered = [ ndi.convolve(X_train[i],kernal) for i in range(X_train.shape[0])]\n",
    "    return np.array(X_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import gabor_kernel\n",
    "from scipy import ndimage as ndi\n",
    "kernels = []\n",
    "for theta in range(4):\n",
    "    theta = theta / 4. * np.pi\n",
    "    for sigma in (1, 3):\n",
    "        for frequency in (0.05, 0.25):\n",
    "            kernel = np.real(gabor_kernel(frequency, theta=theta,sigma_x=sigma, sigma_y=sigma))\n",
    "#             gabor_filter(X_train[:2],k)\n",
    "            kernels.append(kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normaliseFeatures(train.iloc[:-2000,:-10].values)\n",
    "X_train = np.concatenate([X_train,gabor_filter(X_train,kernels[0][1])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft, dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = dct(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normaliseFeatures(train.iloc[:-2000,:-10].values)\n",
    "Y_train = train.iloc[:-2000,-10:].values\n",
    "X_test = normaliseFeatures(train.iloc[-2000:,:-10].values)\n",
    "Y_test = train.iloc[-2000:,-10:].values\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import feature\n",
    "H = feature.hog(X_train[1], orientations=9, pixels_per_cell=(32, 32),cells_per_block=(2, 2), transform_sqrt=True, block_norm=\"L1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([X_train.T,np.array(gabor_filter(X_train,kernels[0][1])).T,np.real(fft(X_train)).T],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normaliseFeatures(train.iloc[:-2000,:-10].values)\n",
    "Y_train = train.iloc[:-2000,-10:].values\n",
    "X_test = normaliseFeatures(train.iloc[-2000:,:-10].values)\n",
    "Y_test = train.iloc[-2000:,-10:].values\n",
    "# X_train = np.concatenate([X_train.T,gabor_filter(X_train,kernels[0][1]).T,np.real(fft(X_train)).T],axis=1)\n",
    "# X_test = np.concatenate([X_test,gabor_filter(X_test,kernels[0][1]),np.real(fft(X_test)).shape],axis=1)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "with open(sys.argv[3]) as f:\n",
    "    param = f.read().split(\"\\n\")\n",
    "    learningType = int(param[0])\n",
    "    learningRate = float(param[1])\n",
    "    maxIteration = int(param[2])\n",
    "    batchSize = int(param[3])\n",
    "    layer = list(map(int,param[4].split(\" \")))\n",
    "    layer.insert(0,X_train.shape[1])\n",
    "    layer.append(Y_train.shape[1])\n",
    "print(learningType,learningRate,maxIteration,batchSize,layer)        \n",
    "k = batchSize\n",
    "l = X_train.shape[0]\n",
    "weights, bias = initialiseWeights(layer)\n",
    "error_value = []\n",
    "o = maxIteration\n",
    "batches = l/k\n",
    "accuracy_test = []\n",
    "accuracy_train = []\n",
    "for i in range(o):\n",
    "    print(i,end=\"\\r\",flush=True)\n",
    "    start_index = int(k*(i%batches))\n",
    "    end_index = int(k*((i%batches)+1))\n",
    "    a_layer , z_layer = feedforward(weights,bias,X_train[start_index:end_index,:])\n",
    "    Y_pred , z = feedforward(weights,bias,X_test)\n",
    "    accuracy_test.append(hot_encode(Y_pred[-1],Y_test.T))\n",
    "#     Y_pred , z = feedforward(weights,bias,X_train)\n",
    "#     accuracy_train.append(hot_encode(Y_pred[-1],Y_train.T))\n",
    "    error_value.append(error(a_layer[-1],Y_train[start_index:end_index,:]))\n",
    "    weights, bias = backpropagate(weights,bias,a_layer,z_layer,X_train[start_index:end_index,:],Y_train[start_index:end_index,:],np.sqrt(1),0)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(error_value)),error_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(accuracy_test)),accuracy_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normaliseFeatures(train.iloc[:-2000,:-10].values)\n",
    "Y_train = train.iloc[:-2000,-10:].values\n",
    "X_test = normaliseFeatures(train.iloc[-2000:,:-10].values)\n",
    "Y_test = train.iloc[-2000:,-10:].values\n",
    "X_train = np.concatenate([X_train,gabor_filter(X_train,kernels[1][1])],axis=1)\n",
    "X_test = np.concatenate([X_test,gabor_filter(X_test,kernels[0][1])],axis=1)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "with open(sys.argv[3]) as f:\n",
    "    param = f.read().split(\"\\n\")\n",
    "    learningType = int(param[0])\n",
    "    learningRate = float(param[1])\n",
    "    maxIteration = int(param[2])\n",
    "    batchSize = int(param[3])\n",
    "    layer = list(map(int,param[4].split(\" \")))\n",
    "    layer.insert(0,X_train.shape[1])\n",
    "    layer.append(Y_train.shape[1])\n",
    "print(learningType,learningRate,maxIteration,batchSize,layer)        \n",
    "k = batchSize\n",
    "l = X_train.shape[0]\n",
    "weights, bias = initialiseWeights(layer)\n",
    "error_value = []\n",
    "o = maxIteration\n",
    "batches = l/k\n",
    "accuracy_test = []\n",
    "accuracy_train = []\n",
    "for i in range(o):\n",
    "    print(i,end=\"\\r\",flush=True)\n",
    "    start_index = int(k*(i%batches))\n",
    "    end_index = int(k*((i%batches)+1))\n",
    "    a_layer , z_layer = feedforward(weights,bias,X_train[start_index:end_index,:])\n",
    "    Y_pred , z = feedforward(weights,bias,X_test)\n",
    "    accuracy_test.append(hot_encode(Y_pred[-1],Y_test.T))\n",
    "#     Y_pred , z = feedforward(weights,bias,X_train)\n",
    "#     accuracy_train.append(hot_encode(Y_pred[-1],Y_train.T))\n",
    "    error_value.append(error(a_layer[-1],Y_train[start_index:end_index,:]))\n",
    "    weights, bias = backpropagate(weights,bias,a_layer,z_layer,X_train[start_index:end_index,:],Y_train[start_index:end_index,:],np.sqrt(1),0)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(error_value)),error_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(accuracy_test)),accuracy_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normaliseFeatures(train.iloc[:-2000,:-10].values)\n",
    "Y_train = train.iloc[:-2000,-10:].values\n",
    "X_test = normaliseFeatures(train.iloc[-2000:,:-10].values)\n",
    "Y_test = train.iloc[-2000:,-10:].values\n",
    "X_train = np.concatenate([X_train,gabor_filter(X_train,kernels[2][1])],axis=1)\n",
    "X_test = np.concatenate([X_test,gabor_filter(X_test,kernels[2][1])],axis=1)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "with open(sys.argv[3]) as f:\n",
    "    param = f.read().split(\"\\n\")\n",
    "    learningType = int(param[0])\n",
    "    learningRate = float(param[1])\n",
    "    maxIteration = int(param[2])\n",
    "    batchSize = int(param[3])\n",
    "    layer = list(map(int,param[4].split(\" \")))\n",
    "    layer.insert(0,X_train.shape[1])\n",
    "    layer.append(Y_train.shape[1])\n",
    "print(learningType,learningRate,maxIteration,batchSize,layer)        \n",
    "k = batchSize\n",
    "l = X_train.shape[0]\n",
    "weights, bias = initialiseWeights(layer)\n",
    "error_value = []\n",
    "o = maxIteration\n",
    "batches = l/k\n",
    "accuracy_test = []\n",
    "accuracy_train = []\n",
    "for i in range(o):\n",
    "    print(i,end=\"\\r\",flush=True)\n",
    "    start_index = int(k*(i%batches))\n",
    "    end_index = int(k*((i%batches)+1))\n",
    "    a_layer , z_layer = feedforward(weights,bias,X_train[start_index:end_index,:])\n",
    "    Y_pred , z = feedforward(weights,bias,X_test)\n",
    "    accuracy_test.append(hot_encode(Y_pred[-1],Y_test.T))\n",
    "#     Y_pred , z = feedforward(weights,bias,X_train)\n",
    "#     accuracy_train.append(hot_encode(Y_pred[-1],Y_train.T))\n",
    "    error_value.append(error(a_layer[-1],Y_train[start_index:end_index,:]))\n",
    "    weights, bias = backpropagate(weights,bias,a_layer,z_layer,X_train[start_index:end_index,:],Y_train[start_index:end_index,:],np.sqrt(1),0)        \n",
    "plt.plot(range(len(error_value)),error_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(accuracy_test)),accuracy_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normaliseFeatures(train.iloc[:-2000,:-10].values)\n",
    "Y_train = train.iloc[:-2000,-10:].values\n",
    "X_test = normaliseFeatures(train.iloc[-2000:,:-10].values)\n",
    "Y_test = train.iloc[-2000:,-10:].values\n",
    "X_train = np.concatenate([X_train,gabor_filter(X_train,kernels[1][1])],axis=1)\n",
    "X_test = np.concatenate([X_test,gabor_filter(X_test,kernels[1][1])],axis=1)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "with open(sys.argv[3]) as f:\n",
    "    param = f.read().split(\"\\n\")\n",
    "    learningType = int(param[0])\n",
    "    learningRate = float(param[1])\n",
    "    maxIteration = int(param[2])\n",
    "    batchSize = int(param[3])\n",
    "    layer = list(map(int,param[4].split(\" \")))\n",
    "    layer.insert(0,X_train.shape[1])\n",
    "    layer.append(Y_train.shape[1])\n",
    "print(learningType,learningRate,maxIteration,batchSize,layer)        \n",
    "k = batchSize\n",
    "l = X_train.shape[0]\n",
    "weights, bias = initialiseWeights(layer)\n",
    "error_value = []\n",
    "o = maxIteration\n",
    "batches = l/k\n",
    "accuracy_test = []\n",
    "accuracy_train = []\n",
    "for i in range(o):\n",
    "    print(i,end=\"\\r\",flush=True)\n",
    "    start_index = int(k*(i%batches))\n",
    "    end_index = int(k*((i%batches)+1))\n",
    "    a_layer , z_layer = feedforward(weights,bias,X_train[start_index:end_index,:])\n",
    "    Y_pred , z = feedforward(weights,bias,X_test)\n",
    "    accuracy_test.append(hot_encode(Y_pred[-1],Y_test.T))\n",
    "#     Y_pred , z = feedforward(weights,bias,X_train)\n",
    "#     accuracy_train.append(hot_encode(Y_pred[-1],Y_train.T))\n",
    "    error_value.append(error(a_layer[-1],Y_train[start_index:end_index,:]))\n",
    "    weights, bias = backpropagate(weights,bias,a_layer,z_layer,X_train[start_index:end_index,:],Y_train[start_index:end_index,:],np.sqrt(1),0)        \n",
    "plt.plot(range(len(error_value)),error_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(accuracy_test)),accuracy_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(X_train[11].reshape(32,32))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import gabor_kernel\n",
    "from scipy import ndimage as ndi\n",
    "kernels = []\n",
    "for theta in range(4):\n",
    "    theta = theta / 4. * np.pi\n",
    "    for sigma in (1, 3):\n",
    "        for frequency in (0.05, 0.25):\n",
    "            kernel = np.real(gabor_kernel(frequency, theta=theta,sigma_x=sigma, sigma_y=sigma))\n",
    "            kernels.append(kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.real(gabor_kernel(2, theta=np.pi/2,sigma_x=1, sigma_y=1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import gabor_kernel\n",
    "from scipy import ndimage as ndi\n",
    "kernels = []\n",
    "for theta in range(4):\n",
    "    theta = theta / 4. * np.pi\n",
    "    for sigma in (1, 3):\n",
    "        for frequency in (0.05, 0.25):\n",
    "            kernel = np.real(gabor_kernel(frequency, theta=theta,sigma_x=sigma, sigma_y=sigma))\n",
    "            gabor_filter(X_train[:2],k)\n",
    "            kernels.append(kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# for i,k in enumerate(kernels):\n",
    "#     print(k.shape)\n",
    "# k = np.real(gabor_kernel(2, theta=np.pi/2,sigma_x=1, sigma_y=1))\n",
    "# print(k.shape)\n",
    "\n",
    "d = ndi.convolve(X_train[11],kernels[1][1])\n",
    "print(d)\n",
    "plt.imshow(d.reshape(32,32),cmap=\"gray\")\n",
    "plt.show()\n",
    "plt.imshow(X_train[11].reshape(32,32),cmap=\"gray\")\n",
    "plt.show()\n",
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.real(gabor_kernel(4, theta=np.pi/4,sigma_x=2, sigma_y=2)).shape\n",
    "gabor_filter(X_train[:2],k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = ndi.convolve(X_train[1],kernel[6])\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(d.reshape(32,32))\n",
    "plt.show()\n",
    "plt.imshow(X_train[1].reshape(32,32),cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.iloc[1,:-10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = normaliseFeatures(train.iloc[:-2000,:-10].values)\n",
    "# Y_train = train.iloc[:-2000,-10:].values\n",
    "# X_test = normaliseFeatures(train.iloc[-2000:,:-10].values)\n",
    "# Y_test = train.iloc[-2000:,-10:].values\n",
    "\n",
    "# learningType = 2\n",
    "# learningRate = float(0.5)\n",
    "# maxIteration = int(1000)\n",
    "# batchSize = int(100)\n",
    "# layer = [300 , 150, 90]\n",
    "# layer.insert(0,X_train.shape[1])\n",
    "# layer.append(Y_train.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(accuracy_test)),accuracy_test)\n",
    "plt.show\n",
    "plt.plot(range(len(accuracy_train)),accuracy_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer , z_layer = feedforward(weights,bias,X_test)\n",
    "hot_encode(a_layer[-1],Y_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer , z_layer = feedforward(weights,bias,X_test)\n",
    "\n",
    "# a = np.array([ np.where(out==np.amax(out))[0][0] for out in a_layer[-1].T])\n",
    "# j=0\n",
    "# for i in range(a.shape[0]):\n",
    "#     if(a[i]==train_class[i+18000]):\n",
    "#         j+=1\n",
    "# print(j/a.shape[0])\n",
    "# plt.hist(a)\n",
    "# plt.show()\n",
    "# plt.plot(range(len(error_value)),error_value)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer , z_layer = feedforward(weights,bias,X_test)\n",
    "a = np.array([ np.where(out==np.amax(out))[0][0] for out in a_layer[-1].T])\n",
    "j=0\n",
    "for i in range(a.shape[0]):\n",
    "    if(a[i]==train_class[i]):\n",
    "        j+=1\n",
    "print(j/a.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer , z_layer = feedforward(weights,bias,X_train)\n",
    "a = np.array([ np.where(out==np.amax(out))[0][0]+1 for out in a_layer[-1].T])\n",
    "j=0\n",
    "for i in range(a.shape[0]):\n",
    "    if(a[i]==train_class[i]):\n",
    "        j+=1\n",
    "j/a.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(error_value)),error_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([])\n",
    "for m in range(len(bias)):\n",
    "    a = np.concatenate((a,bias[m].flatten(),weights[m].flatten('F')))\n",
    "np.savetxt(\"weigth.txt\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_der = -Y_train.T/(a_layer[-1]*Y_train.shape[0])\n",
    "error_der.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = softmax(z_layer[-1])\n",
    "(s*(1-s)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_laye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer[2][a_layer[2]>0.5]=1\n",
    "a_layer[2][a_layer[2]<0.5]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer[2].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'pred':a_layer[2].tolist(),'actual':Y_train.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(a_layer[2],Y_train,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(weights,bias,a_layer,z_layer,X_train,Y_train):\n",
    "    new_weights = []\n",
    "    new_bias = []\n",
    "    error_der = (a_layer[2]-Y_train)/((1-a_layer[2])*a_layer[2]*X_train.shape[0])\n",
    "    s = sigmoid(z_layer[2])\n",
    "    sigmaDer = (s*(1-s))\n",
    "    deltaL = error_der*sigmaDer\n",
    "    new_weights.append(np.subtract(weights[1] ,np.multiply(learningRate,np.matmul(deltaL,a_layer[1].T))))\n",
    "    b = np.sum(deltaL,axis=1)\n",
    "    new_bias.append(np.subtract(bias[1],np.multiply(learningRate,b.reshape(b.shape[0],1))))\n",
    "    s = sigmoid(z_layer[1])\n",
    "    sigmaDer = (s*(1-s))\n",
    "    deltal3 = np.matmul(weights[1].T,deltaL)*sigmaDer\n",
    "    new_weights.append(np.subtract(weights[0] ,np.multiply(learningRate,np.matmul(deltal3,a_layer[0].T))))\n",
    "    b = np.sum(deltal3,axis=1)\n",
    "    new_bias.append(np.subtract(bias[0],np.multiply(learningRate,b.reshape(b.shape[0],1))))\n",
    "    new_bias.reverse()\n",
    "    new_weights.reverse()\n",
    "    return (new_weights,new_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k = batchSize\n",
    "l = X_train.shape[0]\n",
    "maxIteration =30\n",
    "weights, bias = initialiseWeights(layer)\n",
    "error = []\n",
    "for i in range(maxIteration):\n",
    "    start_index = int((k)*(i%6))\n",
    "    end_index = int((k)*((i%6)+1))\n",
    "    a_layer , z_layer = feedforward(weights,bias,X_train[start_index:end_index,:])\n",
    "    weights, bias = backpropagate(weights,bias,a_layer,z_layer,X_train[start_index:end_index,:],Y_train[start_index:end_index])\n",
    "# a_layer , z_layer = feedforward(weights,bias,X_train)\n",
    "# error.append(np.linalg.norm(np.subtract(a_layer[2],Y_train))/300)   \n",
    "for i in range(len(bias)):\n",
    "    print(bias[i].flatten())\n",
    "print(\"errr\")\n",
    "for i in range(len(weights)):\n",
    "    print(weights[i].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer , z_layer = feedforward(weights,bias,X_train[0:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = []\n",
    "new_bias = []\n",
    "# error_der = (a_layer[2]-Y_train)/((1-a_layer[2])*a_layer[2]*X_train.shape[0])\n",
    "# s = sigmoid(z_layer[2])\n",
    "# sigmaDer = (s*(1-s))\n",
    "# deltaL = error_der*sigmaDer\n",
    "error_der = a_layer\n",
    "# print(deltaL.shape)\n",
    "# new_weights.append(np.subtract(weights[1] ,np.multiply(learningRate,np.matmul(deltaL,a_layer[1].T))))\n",
    "# b = np.sum(deltaL,axis=1)\n",
    "# new_bias.append(b.reshape(b.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias = initialiseWeights(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "k = batchSize\n",
    "l = X_train.shape[0]\n",
    "maxIteration =1\n",
    "weights, bias = initialiseWeights(layer)\n",
    "error = []\n",
    "for j in range(maxIteration):\n",
    "#     print(j,end=\"\\r\",flush=True)\n",
    "    for i in range(int(l/k)):\n",
    "        start_index = int((k)*i)\n",
    "        end_index = int((k)*(i+1))\n",
    "        a_layer , z_layer = feedforward(weights,bias,X_train[start_index:end_index,:])\n",
    "        weights, bias = backpropagate(weights,bias,a_layer,z_layer,X_train[start_index:end_index,:],Y_train[start_index:end_index])\n",
    "#         for k in range(len(bias)):\n",
    "#             print(bias[k].flatten(),weights[k].flatten())\n",
    "#         print(\"hi this is akshay\\n\\n\\n\")\n",
    "    a_layer , z_layer = feedforward(weights,bias,X_train)\n",
    "    error.append(np.linalg.norm(np.subtract(a_layer[2],Y_train))/300)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(error)),error)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_layer , z_layer = feedforward(weights,bias,X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_layer[2]+Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias = initialiseWeights(layer)\n",
    "a_layer , z_layer = feedforward(weights,bias,X_train[0:50])\n",
    "weights, bias = backpropagate(weights,bias,a_layer,z_layer,X_train[0:50],Y_train[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bias)):\n",
    "    print(bias[i].flatten())\n",
    "print(\"errr\")\n",
    "for i in range(len(weights)):\n",
    "    print(weights[i].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am = [10 , 20 ,30]\n",
    "40 in am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = []\n",
    "new_bias = []\n",
    "error_der = (a_layer[2]-Y_train)/((1-a_layer[2])*a_layer[2]*X_train.shape[0])\n",
    "s = sigmoid(z_layer[2])\n",
    "sigmaDer = (s*(1-s))\n",
    "deltaL = error_der*sigmaDer\n",
    "print(deltaL.shape)\n",
    "new_weights.append(np.subtract(weights[1] ,np.multiply(learningRate,np.matmul(deltaL,a_layer[1].T))))\n",
    "b = np.sum(deltaL,axis=1)\n",
    "new_bias.append(b.reshape(b.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(new_weights[-1])\n",
    "s = sigmoid(z_layer[1])\n",
    "sigmaDer = (s*(1-s))\n",
    "deltal3 = np.matmul(weights[1].T,deltaL)*sigmaDer\n",
    "new_weights.append(np.subtract(weights[0] ,np.multiply(learningRate,np.matmul(deltal3,a_layer[0].T))))\n",
    "b = np.sum(deltal3,axis=1)\n",
    "new_bias.append(b.reshape(b.shape[0],1))\n",
    "# print(new_weights[0].shape,new_weights[1].shape)\n",
    "# print(np.sum(deltaL),deltal3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bias.reverse()\n",
    "new_weights.reverse()\n",
    "print(new_bias,\"\\n\",new_weights)\n",
    "weights = new_weights\n",
    "bias = new_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sigmoid(z_layer[1])\n",
    "sigmaDer = (s*(1-s))\n",
    "deltal3 = np.matmul(weights[1].T,deltaL)*sigmaDer\n",
    "print(weights[1].shape,deltaL.shape,sigmaDer.shape)\n",
    "new_bias.append(np.sum(deltal3,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(weights[1].T,deltaL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = []\n",
    "new_bias = []\n",
    "error_der = (a_layer[4]-Y_train)/((1-a_layer[4])*a_layer[4]*X_train.shape[0])\n",
    "s = sigmoid(z_layer[4])\n",
    "sigmaDer = (s*(1-s))\n",
    "deltaL = error_der*sigmaDer\n",
    "print(deltaL.shape)\n",
    "new_weights.append(np.subtract(weights[3] ,np.multiply(learningRate,np.matmul(deltaL,a_layer[3].T))))\n",
    "print(new_weights[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sigmoid(z_layer[3])\n",
    "sigmaDer = (s*(1-s))\n",
    "deltal3 = np.matmul(weights[3].T,deltaL)*sigmaDer\n",
    "new_weights.append(np.subtract(weights[2] ,np.multiply(learningRate,np.matmul(deltal3,a_layer[2].T))))\n",
    "print(new_weights[-1])\n",
    "# print(weights[2].shape,deltal3.shape,a_layer[2].T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sigmoid(z_layer[2])\n",
    "sigmaDer = (s*(1-s))\n",
    "deltal2 = np.matmul(weights[2].T,deltal3)*sigmaDer\n",
    "new_weights.append(np.subtract(weights[1] ,np.multiply(learningRate,np.matmul(deltal2,a_layer[1].T))))\n",
    "print(new_weights[-1])\n",
    "# print(weights[2].shape,deltal3.shape,a_layer[2].T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sigmoid(z_layer[1])\n",
    "sigmaDer = (s*(1-s))\n",
    "deltal1 = np.matmul(weights[1].T,deltal2)*sigmaDer\n",
    "new_weights.append(np.subtract(weights[0] ,np.multiply(learningRate,np.matmul(deltal1,a_layer[0].T))))\n",
    "print(new_weights[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltal3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.negative(deltaL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = X_train.T\n",
    "z1 = np.matmul(weights[0],y0)+bias[0]\n",
    "y1 = sigmoid(z1)\n",
    "z2 = np.matmul(weights[1],y1)+bias[1]\n",
    "y2 = sigmoid(z2)\n",
    "z3 = np.matmul(weights[2],y2)+bias[2]\n",
    "y3 = sigmoid(z3)\n",
    "z4 = np.matmul(weights[3],y3)+bias[3]\n",
    "y4 = sigmoid(z4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y0.shape,y1.shape,y2.shape,y3.shape,y4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "train = pd.read_csv(sys.argv[1],header=None)\n",
    "test  = pd.read_csv(sys.argv[2],header=None)\n",
    "classes = pd.get_dummies(train[1024],prefix=\"class_\")\n",
    "train = pd.concat([train.iloc[:,:-1],classes],axis=1)\n",
    "def initialiseWeights(layer):\n",
    "    weights = []\n",
    "    bias = []\n",
    "    for i in range(1,len(layer)):\n",
    "        weights.append((np.random.rand(layer[i-1]*layer[i]).reshape(layer[i],layer[i-1])-0.5))\n",
    "        bias.append((np.random.rand(layer[i]).reshape(layer[i],1)-0.5))\n",
    "    return (weights,bias)    \n",
    "def normaliseFeatures(X_train):\n",
    "    return (X_train-np.mean(X_train,axis=0))/255;\n",
    "X_train = normaliseFeatures(train.iloc[:,:-10].values)\n",
    "Y_train = train.iloc[:,-10:].values\n",
    "X_test  = test.iloc[:,:-1].values\n",
    "print(X_train.shape,Y_train.shape,X_test.shape)\n",
    "learningType = 2\n",
    "learningRate = float(0.01)\n",
    "maxIteration = int(1000)\n",
    "batchSize = int(100)\n",
    "layer = [10]\n",
    "layer.insert(0,X_train.shape[1])\n",
    "layer.append(Y_train.shape[1])\n",
    "print(learningType,learningRate,maxIteration,batchSize,layer)\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp((-1)*z))\n",
    "def sigmoid_der(z):\n",
    "    s = sigmoid(z)\n",
    "    return s*(1-s)\n",
    "def tanh(z):\n",
    "    return (2/(1+np.exp((-2)*z)))-1\n",
    "def tanhder(z):\n",
    "    t = tanh(z)\n",
    "    return 1-t*t;\n",
    "def relu(z):\n",
    "    return np.where(z>0,z,0)\n",
    "def reluder(z):\n",
    "    z[z>0]=1\n",
    "    z[z<0]=0\n",
    "    return z\n",
    "def softmax(z):\n",
    "    z = np.exp(z)\n",
    "    return z/np.sum(z,axis=0)    \n",
    "def feedforward(weights,bias,X_train):\n",
    "    a_layer = []\n",
    "    z_layer = []\n",
    "    a_layer.append(X_train.T)\n",
    "    z_layer.append(0)\n",
    "    for i in range(0,len(layer)-2):\n",
    "        z_layer.append(np.matmul(weights[i],a_layer[i])+bias[i])\n",
    "#         a_layer.append(sigmoid(z_layer[i+1]))\n",
    "        a_layer.append(relu(z_layer[i+1]))\n",
    "#         a_layer.append(tanh(z_layer[i+1]))\n",
    "    z_layer.append(np.matmul(weights[-1],a_layer[-1])+bias[-1])\n",
    "    a_layer.append(softmax(z_layer[-1]))    \n",
    "    return (a_layer,z_layer)    \n",
    "def backpropagate(weights,bias,a_layer,z_layer,X_train,Y_train,iteration):\n",
    "    new_weights = []\n",
    "    new_bias = []\n",
    "    layer_index = len(a_layer)-1\n",
    "    delta = (a_layer[-1]-Y_train.T)/Y_train.shape[0]\n",
    "    new_weights.append(np.subtract(weights[layer_index-1] ,np.multiply((learningRate/iteration),np.matmul(delta,a_layer[layer_index-1].T))))\n",
    "    b = np.sum(delta,axis=1)\n",
    "    new_bias.append(np.subtract(bias[layer_index-1],np.multiply((learningRate/iteration),b.reshape(b.shape[0],1))))\n",
    "    layer_index-=1\n",
    "    while(layer_index>0):\n",
    "#         delta = np.matmul(weights[layer_index].T,delta)*sigmoid_der(z_layer[layer_index])\n",
    "        delta = np.matmul(weights[layer_index].T,delta)*reluder(z_layer[layer_index])\n",
    "#         delta = np.matmul(weights[layer_index].T,delta)*tanhder(z_layer[layer_index])\n",
    "        new_weights.append(np.subtract(weights[layer_index-1] ,np.multiply((learningRate/iteration),np.matmul(delta,a_layer[layer_index-1].T))))\n",
    "        b = np.sum(delta,axis=1)\n",
    "        new_bias.append(np.subtract(bias[layer_index-1],np.multiply((learningRate/iteration),b.reshape(b.shape[0],1))))\n",
    "        layer_index-=1\n",
    "    new_bias.reverse()\n",
    "    new_weights.reverse()\n",
    "    return (new_weights,new_bias)    \n",
    "k = batchSize\n",
    "l = X_train.shape[0]\n",
    "weights, bias = initialiseWeights(layer)\n",
    "error_value = []\n",
    "o = maxIteration\n",
    "batches = l/k\n",
    "for i in range(o):\n",
    "    start_index = int(k*(i%batches))\n",
    "    end_index = int(k*((i%batches)+1))\n",
    "    a_layer , z_layer = feedforward(weights,bias,X_train[start_index:end_index,:])\n",
    "    # error_value.append(error(a_layer[-1],Y_train[start_index:end_index,:]))\n",
    "    weights, bias = backpropagate(weights,bias,a_layer,z_layer,X_train[start_index:end_index,:],Y_train[start_index:end_index,:],np.sqrt(1))        \n",
    "    \n",
    "a_layer , z_layer = feedforward(weights,bias,X_test)\n",
    "a = np.array([ np.where(out==np.amax(out))[0][0] for out in a_layer[-1].T])\n",
    "np.savetxt(sys.argv[3],a)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
